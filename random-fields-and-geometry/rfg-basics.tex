\documentclass{article}

\input{preamble.sty}
\usepackage{lmodern}

\newcommand{\ag}[1]{{\bf{{\red{[{AG: #1}]}}}}}
\newcommand{\InnerProduct}[2]{\langle #1,#2 \rangle}
\newcommand{\Norm}[1]{\|#1\|}
\newcommand{\bP}{\mathbb{P}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\M}{\mathbb{M}}
\newcommand{\V}{\mathbb{V}}
\newcommand{\br}{{\bf r}}
\newcommand{\bs}{{\bf s}}
\newcommand{\bt}{{\boldsymbol t}}
\newcommand{\bH}{{\boldsymbol H}}
\newcommand{\Newton}{\mathrm{Newton}}
\newcommand{\DummyField}{{\tt f}}
\newcommand{\DummyGradient}{{\bf g}}
\newcommand{\InformationMatrix}{{\boldsymbol J}}
\newcommand{\lin}{\mathrm{lin}}
\newcommand{\ind}{\perp\!\!\!\!\perp} 
\newcommand{\Exp}{\mathrm{Exp}}

\newcommand{\RandomField}{Y}
\newcommand{\bLambda}{{\boldsymbol \Lambda}}
\newcommand{\bGamma}{{\boldsymbol \Gamma}}
\newcommand{\Err}{\mathrm{Err}}
\newcommand{\bQ}{{\boldsymbol Q}}
\newcommand{\bJ}{{\boldsymbol J}}
\newcommand{\bV}{{\boldsymbol V}}
\newcommand{\bI}{{\boldsymbol I}}
\newcommand{\bC}{{\boldsymbol C}}
\newcommand{\convweak}{\overset{d}{\to}}

\newcommand{\appropto}{\mathrel{\vcenter{
			\offinterlineskip\halign{\hfil$##$\cr
				\propto\cr\noalign{\kern2pt}\sim\cr\noalign{\kern-2pt}}}}}

\renewcommand{\thealgorithm}{\arabic{algorithm}}

\title{ {\bf Notes on Random Fields and Geometry} }

\begin{document}
	
	\maketitle
	\RaggedRight
	
	Consider a generic signal-plus-noise random field
	\begin{equation}
		\label{eqn:signal-plus-noise}
		Y_t = \mu_t + \epsilon_t, \quad {\rm for} ~ t \in \mc{T},
	\end{equation}
	where $\mc{T}$ is a manifold, possibly with boundary. Our ultimate interest is in understanding the distribution of critical points of $Y$, possibly under side conditions (e.g. local maxima, global maxima, etc.) An important formula for doing so is the Kac-Rice formula. These notes explain the Kac-Rice theorem for manifolds, and provide some examples of random fields to which it applies. In order to get to this point, we start by reviewing some of the necessary basics of differential geometry.
	
	\section{Basics of differential geometry}
	
	We adopt the modern view on differential geometry -- in which manifolds are abstractly defined rather than being  embedded in Euclidean space -- occasional lapsing to the traditional view when a more physical understanding is helpful to build intuition. 
	
	\subsection{Modern viewpoint}
	  
	\paragraph{Manifolds.}
	A $d$-dimensional \emph{manifold} $M$ is a \red{locally compact Hausdorff} space, such that for every $t \in M$ there exists an open $U \subset M$ with $t \in U$ such that $U$ is homeomorphic to an open $\tilde{U} \in \Rd$. For such a manifold, a \emph{coordinate chart} is a pair $(\varphi,U)$ where $U$ is open, and $\varphi: U \to \varphi(U)$ is a homeomorphism. We call $(x_1(t),\ldots,x_d(t)) = \varphi(t)$ the \emph{local coordinates} on $U$.
	
	\paragraph{Tangent vectors and derivations.}
	There is a very obvious and intuitive definition of tangent vectors and tangent spaces of an embedded manifold $M \subseteq \Rd$ that is reviewed in Section~\ref{subsec:traditional-viewpoint}. These vectors can be associated with directional derivatives in an obvious way. The abstraction of tangent vectors to abstract (not embedded) manifolds comes from generalizing the notion of directional derivatives to what are called derivations.
	
	A \emph{derivation} $X_p$ of a function $f \in C^{\infty}(M)$ at a point $p$ satisfies the product (Liebniz) rule and linearity:
	\begin{equation*}
	\begin{aligned}
		X_p(fg) & = X_p(f)g + f X_p(g) \\
		X_p(af + b g) &= a X_p(f) + b X_p(g), \quad {\rm for} a,b \in \R.
	\end{aligned}
	\end{equation*}
	When equipped with the obvious operations of addition and scaling, the space of derivations is a vector space of dimension $d = \dim M$, which we call the \emph{tangent space} to $M$ at $p$, and denote by $T_p(M)$.
	
	\paragraph{Coordinate-wise derivatives.}
	We now detail how derivations are represented in local coordinates. This is accomplished by taking the standard basis of $T_t(\Rd)$ and ``pushing forward'' to a basis of $T_{p}(M)$. 
	
	We start with the trivial case where $M = \Rd$ and all we are doing is redefining terms. We give $M$ coordinates via the trivial chart $(\varphi,M)$, where $\varphi(t) = t$ is the inclusion map. At a given point $t$, we use the notation
	\begin{equation*}
		\bigg(\frac{\partial}{\partial x_i}\Big|_{t}\bigg)f := \frac{\partial f}{\partial x_i}(t),
	\end{equation*}
	to refer to the partial derivatives of $f$ at point $t$. The set $\{\frac{\partial}{\partial x_i}|_{t}:i = 1,\ldots,d\}$ forms a basis of $T_t(\Rd)$, and thus any derivation $X_t \in T_t(\Rd)$ can be represented as 
	\begin{equation}
	\label{eqn:tangent-vector-in-local-coordinates}
		X_t := \sum_{i = 1}^{d} a^i \frac{\partial}{\partial x_i}\Big|_{t}
	\end{equation}
	Now suppose $M$ is an abstract manifold, and consider the chart $(\varphi,U)$. For a given $p \in U$, the \emph{push-forward} $g_{\ast}: T_{\varphi(p)}(\Rd) \to T_{p}(M)$ of $\varphi^{-1}$ is defined by 
	\begin{equation}
	\label{eqn:push-forward}
		\bigg(g_{\ast}(X_{\varphi(p)})\bigg)f := X_{\varphi(p)}(f \circ \varphi^{-1})
	\end{equation}
	Now pushing forward the standard basis vectors results in the set
	\begin{equation}
	\label{eqn:natural-basis}
		\bigg\{g_{\ast}\Big(\frac{\partial}{\partial x_i}\Big|_{\varphi(p)}\Big): i = 1,\ldots, d\bigg\}.
	\end{equation}
	It can be shown that $g_{\ast}$ is injective and linear, and therefore~\eqref{eqn:natural-basis} is a basis of $T_p(M)$, called the \emph{natural basis}. The standard notation used is to denote $\frac{\partial}{\partial x_i}|_{p} := g_{\ast}(\frac{\partial}{\partial x_i}|_{\varphi(p)})$.\footnote{This is not strictly speaking an abuse of notation, since we have never before used the notation $\frac{\partial}{\partial x_i}|_{p}$ when $p$ is a point at an abstract manifold. Nevertheless, it may take you a while to get used to it.} Thus every derivation $X_p \in T_p(M)$ can be written in local coordinates as 
	\begin{equation}
	\label{eqn:derivation-in-local-coordinates}
	X_p := \sum_{i = 1}^{d} a^i \frac{\partial}{\partial x_i}\Big|_{p},
	\end{equation}
	which is almost exactly the same as~\eqref{eqn:tangent-vector-in-local-coordinates}. 
	
	\paragraph{Aside: push-forward as the chain rule.} As a bit of useful intuition, one should think of~\eqref{eqn:push-forward} as generalizing the chain rule of calculus. Suppose $\varphi:\Rd \to \Rd$ was simply a change of coordinates of $\Rd$. In that case, the push-forward of $\partial/\partial x_i|_{\varphi(t)}$ lives in $T_{t}(\Rd)$, and can be represented as a linear combination of the standard basis vectors:
	\begin{equation*}
		g_{\ast}\Big(\frac{\partial}{\partial x_i}\Big|_{\varphi(t)}\Big) = \sum_{i = 1}^{d} b^i \frac{\partial}{\partial x_i}\Big|_{t}.
	\end{equation*}
	Consequently, for a given directional derivative $\frac{\partial}{\partial x_j}|_{\varphi(t)}$, equation~\eqref{eqn:push-forward} reads
	\begin{equation*}
		\sum_{i = 1}^{d} b^i \frac{\partial f}{\partial x_i}(t) = \frac{\partial}{\partial x_i}\Big|_{\varphi(t)}\big(f \circ \varphi^{-1}\big),
	\end{equation*}
	which is the chain rule of calculus in local coordinates $b^i = \frac{\partial \varphi^{-1}}{\partial x_i}(\varphi(t))$. 
	
	\paragraph{Vector field.}
	A \emph{vector field} $X$ is a map that assigns a tangent vector $X_t$ to each point $t \in M$. In a moment we will define bundles and sections, and see that a vector field is just a section of the tangent bundle. Notationally, we write $Xf: M \to \Reals$ for the function 
	\begin{equation*}
		(Xf)_{t} := X_tf.
	\end{equation*}
	Given a coordinate chart $(\varphi,U)$ with local coordinates $(x_1(t),\ldots,x_d(t))$ the vector field $X$ can be written in terms of the basis $\partial/\partial x_i|_{t}$, as 
	\begin{equation*}
		X_t = \sum_{i = 1}^{d} a_i(t) \cdot \frac{\partial}{\partial x_i} \Big|_{t}. 
	\end{equation*}
	Smoothness of vector fields corresponds to smoothness of the functions $a_1(t),\ldots,a_d(t)$.
	
	\paragraph{Vector bundle, section, tangent bundle.} 
	Just as vector fields assign a vector to each point in a manifold, we would like a way of assigning a vector space (e.g. tangent space) to each point in a manifold. This is what a vector bundle does.  Formally speaking, a \emph{vector bundle} is a triple $(E,M,F)$ -- where $M$ is the $d$ dimensional \emph{base manifold}, $F$ is a $q$-dimensional vector space called a \emph{fiber}, and $E$ is a $d + q$ dimensional manifold that locally ``looks like'' a product of $M$ and $F$ -- along with a assignment map $\pi: E \to M$ that basically assigns pairs $(t,v_t) \in E$ to $t$.  A \emph{section} is then a mapping $s: M \to E$.
	
	Intuitively, we should think of a vector bundle as assigning the vector space $\pi^{-1}(t)$ to each $t \in M$, and a section $s$ as a rule for picking an element of $\pi^{-1}(t)$ for each $t \in M$. According to this intuition, the \emph{tangent bundle} $T(M)$ simply assigns $T_t(M)$ to each $t$. A vector field is then a section of the tangent bundle. The cool thing about a vector bundle is that it inherits a differential structure from its base manifold, and is thus itself also a manifold. 
	
	If the fiber $F$ is not a vector space, then the above construction is called a \emph{fiber bundle}.
	
	\paragraph{Tensors, alternating and symmetric tensors, $k$-forms.}
	A tensor is a (multi)linear operator on a vector space. Formally, a map $\omega$ is called a \emph{(n,m)-tensor} if 
	$$
	\omega \in L(\underbrace{V \oplus V \cdots V}_{n} \oplus \underbrace{V^{*} \oplus \cdots V^*}_{m};\R)
	$$ 
	where $V^*$ is the dual space of $V$, and $L(E;\R)$ is the space of multilinear operators mapping $E$ to $\R$, i.e. operators that are linear in each variable. The \emph{covariant} order is $n$, and the \emph{contravariant} order is $m$. Tensors are the fundamental building blocks of much Riemmanian geometry, since they will allow us to define things like length, angle, and derivative of vectors and vector fields. 
	
	Order $(k,0)$-tensors are referred to as \emph{covariant} tensors. An \emph{alternating} $k$-tensor $\omega$ is a covariant tensor that satisfies
	\begin{equation*}
		\gamma(v_{\omega(1)},\ldots,v_{\omega(k)}) = \sign(\omega) \cdot \gamma(v_{1},\ldots,v_{k}),
	\end{equation*}
	for all permutations $\omega \in S(k)$, while a \emph{symmetric} $k$-tensor is a covariant tensor that satisfies
	\begin{equation*}
		\gamma(v_{\omega(1)},\ldots,v_{\omega(k)}) = \gamma(v_{1},\ldots,v_{k}).
	\end{equation*}
	For a vector space $V$, we denote by $\Lambda^k(V)$ and ${\rm Sym}(\mc{T}_0^{k}(V))$ the space of alternating and symmetric $k$-tensors, respectively. 
	
	Just as we wanted a way of assigning a vector (space) to each point $t \in M$, we will want a way of assigning a tensor (space) to each $t \in M$. The objects which do this are called \emph{tensor fields} and \emph{tensor bundles}. An alternating tensor field maps each point $t \in M$ to a tensor in $\Lambda^{k}(T_t(M))$. The same is true for symmetric tensor fields. \emph{Tensor bundles} are collections of tensor fields equipped with an appropriate differential structure; for instance
	$$
	\Lambda^k(M) = \cup_{t \in M} \Lambda^k(T_t(M)).
	$$
	Finally a \emph{(differential) k-form} is a section of $\Lambda^k(M)$ (meaning that it can be identified with a tensor field.)
	
	\paragraph{Riemmanian manifold.}
	A \emph{Riemannian manifold} is a manifold equipped with a Riemmanian metric. A \emph{Riemannian metric} is an order-2 covariant symmetric tensor field $g$ that is also positive definite; that is
	$$
	g_t(X_t,Y_t) = g_t(Y_t,X_t), \quad g_t(X_t,X_t) \geq 0, \quad g_t(X_t,X_t) = 0 \implies X_t = 0.
	$$
	
	\paragraph{Sphere bundle, orthonormal frame bundle.}
	Riemannian manifolds have a notion of length and angle, allowing us to introduce an important pair of fiber bundles. The \emph{sphere bundle} is the fiber bundle of all unit length vectors: that is, it consists of all point,vector pairs $(t,X_t)$ such that $g_t(X_t,X_t) = 1$.  The orthonormal frame bundle consists of all point/orthonormal basis pairs, that is, it consists of $(t,X_{t}^1,\ldots,X_{t}^d)$ such that $X_t^1,\ldots,X_t^d$ is an orthonormal basis of $T_t(M)$. 
	
	\paragraph{Covariant derivative for surfaces.}
	Covariant derivative is the ``correct'' way of defining differentiation of one vector field $Y$ in the direction of another vector field $X$, correct in the sense of taking into account the curvature of $M$. However there are some subtleties involved in defining this notion of differentiation. As in the case of tangent vectors, good intuition is built by first considering the case where $M$ is embedded in $\Rd$.
	
	Suppose $M$ is a surface, i.e. a manifold embedded in $\Rd$. For a point $t \in M$, we are interested in the derivative of a vector field $Y$ in the direction of another field $X$, at a point $t \in M$. There are a few issues, the most basic being that moving in the direction of $X$ may take us off $M$.  A ``first principles'' way of accounting for this would be to define a curve $c: [-\delta,\delta] \to U$ centered at $t$ in the direction $X_t$ -- formally, $c(0) = t, \dot{c}(0) = X_t$ -- and consider the instantaneous rate of change. The second issue is that $Y_{c(\delta)}$ and $Y_{(c(0))}$ are in different tangent spaces, and therefore cannot be subtracted; but this is easily fixable by identifying both tangent spaces with $\Rd$. 
	
	\paragraph{Connection, Levi-Civit\'{a} connection.}  
	
	\ag{TO COME}

	
	
	
	\begin{itemize}
		\item Covariant derivative, gradient.
		\item Connection,  Levi-Civit\'{a} connection, Christoffel symbols.
		\item Covariant Hessian.
		\item Exponential map.
		\item Geodesics.
		\item Normal coordinates.
	\end{itemize}
	
	
	\subsection{Traditional viewpoint for curves}
	\label{subsec:traditional-viewpoint}
	
	This section specializes the above development to the case of a one-dimensional manifold embedded in $\Rd$. Specifically, we will assume that as a set, $N = \eta(M)$ is given by the graph of a (smooth) function $\eta: M \to \Rd$, and $M = [0,1]$. In this case $M$ comes equipped with a canonical coordinates (given by the inclusion map) and additionally inherits a metric from the Euclidean metric in $\Rd$.
	
	Notation: we write $\dot{\eta}(\theta) = (d/d\theta \eta_1,\ldots,d/d\theta \eta_d)$, and likewise for $\ddot{\eta}(\theta)$.  
	
	\paragraph{Tangent vectors.}
	For a given $p = \eta(\theta), \theta \in \R$ and a vector $v \in \Rd$, we say $v$ is tangent to $N$ at $p$ if there exists a curve $\gamma: (-\delta,\delta) \to N$ such that $\dot{\gamma}(0) = v$. At a given point $p \in M, p = \eta(\theta)$, the vector $\dot{\eta}_{\theta}$ is tangent to $N$. The tangent space $T_{p}(N) \subseteq T_{p}(\Rd)$ can be identified with the one-dimensional subspace of $\Rd$ spanned by $\dot{\eta}_{\theta}$. We can identify tangent vectors with derivations in local coordinates: at a point $p = \eta_{\theta} \in M$ we define
	\begin{equation}
	\label{eqn:curve-derivation}
		\dot{\eta}_{\theta}(f) := \frac{d}{d\theta} (f \circ \eta)\Big|_{\eta(\theta) = p},
	\end{equation}
	and extend this to general $v_{\theta} \in T_{p}(N)$ by linearity. Vector fields $V \in T(N)$ are then of the form $V_{\theta} := a(\theta) \cdot \dot{\eta}_{\theta}$. If $f$ is a function of $\Rd$ (or can be smoothly extended to one), then by the chain rule
	\begin{equation*}
		\dot{\eta}_{\theta}(f) = \dotp{\dot{\eta}_{\theta}}{\partial f},
	\end{equation*}
	where $\partial f$ is the gradient of $f$ in $\Rd$. 
	
	\paragraph{Metric.} As promised, $N$ (and therefore $M$) will inherit a metric from its embedding into $\Rd$. Specifically, we define
	\begin{equation*}
		g_{\theta}(\frac{d}{\,d\theta}, \frac{d}{\,d\theta}) = \hat{g}_{p}(\dot{\eta}_{\theta},\dot{\eta}_{\theta}) =  \dotp{\dot{\eta}_{\theta}}{\dot{\eta}_{\theta}}.
	\end{equation*}

	\paragraph{Gradient, covariant derivative.} 
	The (Riemannian)-gradient $\nabla f$ of a function $f \in C^1(N)$ is
	$$
	\nabla f_p = \frac{\dotp{\partial f_p}{\dot{\eta}_{\theta}}}{\dotp{\dot{\eta}_{\theta}}{\dot{\eta}_{\theta}}} \dot{\eta}_{\theta}.
	$$
	The covariant derivative $\nabla f$ is the tensor field 
	\begin{equation*}
		(\nabla f(v))_{p} := v_p(f) = \dotp{\nabla f}{v_p}.
	\end{equation*}
	Notice that gradient and covariant derivative are the same in the sense that $\nabla f(v) = \dotp{\nabla f_p}{v}$. 
	
	\paragraph{Covariant derivative of a vector field.}
	We now specify how to differentiate a vector field $v(\theta) = a(\theta) \dot{\eta}_{\theta}$ in the direction of $\dot{\eta}_{\theta}$. An obvious choice is to simply differentiate $v$ with respect to $\theta$,
	\begin{equation*}
		\frac{d}{d\theta} (v) = a'(\theta) \dot{\eta}_{\theta} + a(\theta) \ddot{\eta}_{\theta},
	\end{equation*}
	but since $\ddot{\eta}_{\theta}$ is not in $T_{\eta_{\theta}}(N)$ this would lead to the undesirable result $(\nabla_{\dot{\eta}} v)$ is not a tangent vector field. Instead we take this and project it onto $T_{\eta_{\theta}}(M)$, leaving
	\begin{equation*}
		(\nabla_{\dot{\eta}} v)(\theta) := a'(\theta) \dot{\eta}_{\theta} + a(\theta) \frac{\dotp{\ddot{\eta}_{\theta}}{\dot{\eta}_{\theta}}}{\dotp{\dot{\eta}_{\theta}}{\dot{\eta}_{\theta}}} \dot{\eta}_{\theta} = a'(\theta) \dot{\eta}_{\theta} + \Gamma_{\theta} v(\theta). 
	\end{equation*}
	The term $\Gamma_{\theta} = \frac{\dotp{\ddot{\eta}_{\theta}}{\dot{\eta}_{\theta}}}{\dotp{\dot{\eta}_{\theta}}{\dot{\eta}_{\theta}}}$ is the Christoffel symbol. It could also have been written $\Gamma_{\theta} = \frac{1}{2}\frac{g'(\theta)}{g(\theta)}$, where $g(\theta) := g_{\theta}(d/d\theta,d/d\theta)$, showing more cleanly how it depends on both metric and coordinates, but not on the particular embedding into $\Rd$ (except through the metric).
	
	\paragraph{Covariant Hessian.}
	The covariant Hessian is defined by
	\begin{equation}
		\label{eqn:covariant-hessian}
		\nabla^2 f(v,v') = v' \partial^2 f v - (\nabla_v v')f.
	\end{equation}
	That is, it takes the second-order partial derivative of $f$ in the direction $(v,v')$ and subtracts the contributions that come from ``incorrectly'' differentiating $v'$ in the direction $v$. 
	
	Notice that we could have also defined $\nabla^2 f = \nabla(\nabla f)$ -- although notice we have not yet defined what it means to take covariant derivative of a tensor. 
	
	
	\section{Examples}
	
	\subsection{Euclidean}
	
	\subsection{Curved exponential family}
	
	\subsection{Kac-Rice test statistic}
	
\end{document}